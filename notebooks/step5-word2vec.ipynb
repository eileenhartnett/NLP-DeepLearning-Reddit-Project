{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import zeros\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import multiprocessing\n",
    "\n",
    "# Word2Vec\n",
    "import gensim \n",
    "import gensim.models\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv('../data/posts-preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>timeframe</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>word_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>sub30605</td>\n",
       "      <td>1499390694</td>\n",
       "      <td>bulimia</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>chest anyone else experience chest purging kno...</td>\n",
       "      <td>['chest anyone else experience chest purging k...</td>\n",
       "      <td>['chest', 'anyone', 'else', 'experience', 'che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>sub27274</td>\n",
       "      <td>1499060654</td>\n",
       "      <td>bulimia</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>dying eat eating die study shifting coping men...</td>\n",
       "      <td>['dying eat eating die study shifting coping m...</td>\n",
       "      <td>['dying', 'eat', 'eating', 'die', 'study', 'sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>sub6055</td>\n",
       "      <td>1499029087</td>\n",
       "      <td>bulimia</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>without purging way lose weight without exercise</td>\n",
       "      <td>['without purging way lose weight without exer...</td>\n",
       "      <td>['without', 'purging', 'way', 'lose', 'weight'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>sub40365</td>\n",
       "      <td>1498978259</td>\n",
       "      <td>bulimia</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>melancholy little month since slowly losing ha...</td>\n",
       "      <td>['melancholy little month since slowly losing ...</td>\n",
       "      <td>['melancholy', 'little', 'month', 'since', 'sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>sub49857</td>\n",
       "      <td>1498814187</td>\n",
       "      <td>bulimia</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>relapsing upset right twice week good tired cy...</td>\n",
       "      <td>['relapsing upset right twice week good tired ...</td>\n",
       "      <td>['relapsing', 'upset', 'right', 'twice', 'week...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author  created_utc subreddit  timeframe  \\\n",
       "0  sub30605   1499390694   bulimia  pre-covid   \n",
       "1  sub27274   1499060654   bulimia  pre-covid   \n",
       "2   sub6055   1499029087   bulimia  pre-covid   \n",
       "3  sub40365   1498978259   bulimia  pre-covid   \n",
       "4  sub49857   1498814187   bulimia  pre-covid   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  chest anyone else experience chest purging kno...   \n",
       "1  dying eat eating die study shifting coping men...   \n",
       "2   without purging way lose weight without exercise   \n",
       "3  melancholy little month since slowly losing ha...   \n",
       "4  relapsing upset right twice week good tired cy...   \n",
       "\n",
       "                                         sent_tokens  \\\n",
       "0  ['chest anyone else experience chest purging k...   \n",
       "1  ['dying eat eating die study shifting coping m...   \n",
       "2  ['without purging way lose weight without exer...   \n",
       "3  ['melancholy little month since slowly losing ...   \n",
       "4  ['relapsing upset right twice week good tired ...   \n",
       "\n",
       "                                         word_tokens  \n",
       "0  ['chest', 'anyone', 'else', 'experience', 'che...  \n",
       "1  ['dying', 'eat', 'eating', 'die', 'study', 'sh...  \n",
       "2  ['without', 'purging', 'way', 'lose', 'weight'...  \n",
       "3  ['melancholy', 'little', 'month', 'since', 'sl...  \n",
       "4  ['relapsing', 'upset', 'right', 'twice', 'week...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **Continuous Bag of Words (CBOW) model**: CBOW model predicts the current word given context words within specific window. The input layer contains the context words and the output layer contains the current word. The hidden layer contains the number of dimensions in which we want to represent current word present at the output layer.\n",
    "\n",
    "https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many cores am I working with?\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in this computer\n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters: \n",
    "- size: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n",
    "- window: (default 5) The maximum distance between a target word and words around the target word.\n",
    "- min_count: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
    "- workers: (default 3) The number of threads to use while training.\n",
    "- sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code help from:\n",
    "- https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "- https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec algorithm processes documents sentence by sentence. This means we will preserve the sentence-based structure during cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>timeframe</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>word_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>sub30605</td>\n",
       "      <td>1499390694</td>\n",
       "      <td>bulimia</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>chest anyone else experience chest purging kno...</td>\n",
       "      <td>['chest anyone else experience chest purging k...</td>\n",
       "      <td>['chest', 'anyone', 'else', 'experience', 'che...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author  created_utc subreddit  timeframe  \\\n",
       "0  sub30605   1499390694   bulimia  pre-covid   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  chest anyone else experience chest purging kno...   \n",
       "\n",
       "                                         sent_tokens  \\\n",
       "0  ['chest anyone else experience chest purging k...   \n",
       "\n",
       "                                         word_tokens  \n",
       "0  ['chest', 'anyone', 'else', 'experience', 'che...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a word2vec model on reddit posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of sentences using Gensim's pre-processing on posts, so that the input yields one sentence (list of utf8 words) after another. https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in posts['sent_tokens']:\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MyCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=sentences, \n",
    "                               window = 10,\n",
    "                               size = 150,\n",
    "                               min_count = 2,\n",
    "                               workers=cores-1) # run on all cores minus 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is fit, we print the size of the learned vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 16910\n"
     ]
    }
   ],
   "source": [
    "# summarize vocabulary size in model\n",
    "vocab = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the learned embedding vectors to file using the save_word2vec_format() on the model’s ‘wv‘ (word vector) attribute. The embedding is saved in ASCII format with one word and vector per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../embedding_word2vec.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model in ASCII (word2vec) format\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
