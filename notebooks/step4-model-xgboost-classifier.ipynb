{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of notebook, from Nature paper will classify each subreddit... etc.\n",
    "\n",
    "> Modeling steps in this notebook\n",
    "- TF-IDF Transform \n",
    "- Split data to training (80%) and testing (20%)\n",
    "- To handle imbalanced class of our target variable we use SMOTE algorithm on the training data\n",
    "- Use XGBoost Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Train, test, split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# For handling imbalanced classes\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# For classification\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/posts-preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>timeframe</th>\n",
       "      <th>words</th>\n",
       "      <th>word_stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>sub30605</td>\n",
       "      <td>1499390694</td>\n",
       "      <td>bulimia</td>\n",
       "      <td>['chest pains  anyone else experience chest pa...</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>['[', \"'chest\", 'pains', 'anyone', 'else', 'ex...</td>\n",
       "      <td>['[', \"'chest\", 'pains', 'anyone', 'else', 'ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>sub27274</td>\n",
       "      <td>1499060654</td>\n",
       "      <td>bulimia</td>\n",
       "      <td>['dying to eat eating to die study on shifting...</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>['[', \"'dying\", 'eat', 'eating', 'die', 'study...</td>\n",
       "      <td>['[', \"'dying\", 'eat', 'eating', 'die', 'study...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>sub6055</td>\n",
       "      <td>1499029087</td>\n",
       "      <td>bulimia</td>\n",
       "      <td>['without purging  what is the quickest way to...</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>['[', \"'without\", 'purging', 'quickest', 'way'...</td>\n",
       "      <td>['[', \"'without\", 'purging', 'quickest', 'way'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>sub40365</td>\n",
       "      <td>1498978259</td>\n",
       "      <td>bulimia</td>\n",
       "      <td>['bulimia and melancholy feelings  i havent pu...</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>['[', \"'bulimia\", 'melancholy', 'feelings', 'h...</td>\n",
       "      <td>['[', \"'bulimia\", 'melancholy', 'feelings', 'h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>sub49857</td>\n",
       "      <td>1498814187</td>\n",
       "      <td>bulimia</td>\n",
       "      <td>['im relapsing  fuck im so upset at myself rig...</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>['[', \"'im\", 'relapsing', 'fuck', 'im', 'upset...</td>\n",
       "      <td>['[', \"'im\", 'relapsing', 'fuck', 'im', 'upset...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author  created_utc subreddit  \\\n",
       "0  sub30605   1499390694   bulimia   \n",
       "1  sub27274   1499060654   bulimia   \n",
       "2   sub6055   1499029087   bulimia   \n",
       "3  sub40365   1498978259   bulimia   \n",
       "4  sub49857   1498814187   bulimia   \n",
       "\n",
       "                                                text  timeframe  \\\n",
       "0  ['chest pains  anyone else experience chest pa...  pre-covid   \n",
       "1  ['dying to eat eating to die study on shifting...  pre-covid   \n",
       "2  ['without purging  what is the quickest way to...  pre-covid   \n",
       "3  ['bulimia and melancholy feelings  i havent pu...  pre-covid   \n",
       "4  ['im relapsing  fuck im so upset at myself rig...  pre-covid   \n",
       "\n",
       "                                               words  \\\n",
       "0  ['[', \"'chest\", 'pains', 'anyone', 'else', 'ex...   \n",
       "1  ['[', \"'dying\", 'eat', 'eating', 'die', 'study...   \n",
       "2  ['[', \"'without\", 'purging', 'quickest', 'way'...   \n",
       "3  ['[', \"'bulimia\", 'melancholy', 'feelings', 'h...   \n",
       "4  ['[', \"'im\", 'relapsing', 'fuck', 'im', 'upset...   \n",
       "\n",
       "                                          word_stems  \n",
       "0  ['[', \"'chest\", 'pains', 'anyone', 'else', 'ex...  \n",
       "1  ['[', \"'dying\", 'eat', 'eating', 'die', 'study...  \n",
       "2  ['[', \"'without\", 'purging', 'quickest', 'way'...  \n",
       "3  ['[', \"'bulimia\", 'melancholy', 'feelings', 'h...  \n",
       "4  ['[', \"'im\", 'relapsing', 'fuck', 'im', 'upset...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binarize targets using get_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use each subreddit as target (except mental health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='subreddit_mentalhealth', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize using TFIDF\n",
    "Implement Term Frequency - Inverse Document Frequency (TF-IDF) to vectorize the pre-processed text from the subreddit posts into numerical representations in a weight matrix that will be the basis for our set of feature for the predictive models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85338, 99061)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit: Anorexia Nervosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit_AnorexiaNervosa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85338,)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample the minority class using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 60478, 1: 7792})\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42) \n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target after oversampling the minority class using the SMOTE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 60478, 1: 60478})\n"
     ]
    }
   ],
   "source": [
    "counter_resampled = Counter(y_train_sm) \n",
    "print(counter_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the re-sampled features to Dmatrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train_sm, label=y_train_sm)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "parameters = {\n",
    "     \"eta\"    : [0.05, 0.1, 0.2, 0.3 ] ,\n",
    "     \"max_depth\"        : [ 1, 3, 5, 7, 11],\n",
    "     \"gamma\"            : [ 0.0, 0.2 , 0.4 ],\n",
    "     }\n",
    "\n",
    "grid = GridSearchCV(clf,\n",
    "                    parameters, n_jobs=4,\n",
    "                    scoring=\"neg_log_loss\",\n",
    "                    cv=3)\n",
    "\n",
    "grid.fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Parameters for XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is num_class? can we run multi-class classification with all subreddits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:41:56] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.15%\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if that is overfit and set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth': 6, # default is 6\n",
    "    'objective': 'multi:softmax', #  logistic regression for binary classification, output score before logistic transformation  \n",
    "    'num_class': 2} \n",
    "\n",
    "steps = 5  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:24:42] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.0\n",
      "Recall = 0.0\n",
      "F1-Score: Non-anorexia = 0.9402378069603551\n",
      "F1-Score: Anorexia = 0.0\n",
      "Accuracy = 0.8872158425123037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds]) ### explain\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds))) ## explain.. adjusting averag='macro' changes this.. look into more\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds))) ### explain\n",
    "print(\"F1-Score: Non-anorexia = {}\".format(f1_score(y_test, best_preds, pos_label=0)))\n",
    "print(\"F1-Score: Anorexia = {}\".format(f1_score(y_test, best_preds, pos_label=1)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit: Anxiety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit_Anxiety']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85338,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 60721, 1: 7549})\n"
     ]
    }
   ],
   "source": [
    "# summarize class distribution\n",
    "counter = Counter(y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample the minority class using SMOTE\n",
    "smote = SMOTE(random_state=42) \n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 60721, 1: 60721})\n"
     ]
    }
   ],
   "source": [
    "# summarize class distribution\n",
    "counter_resampled = Counter(y_train_sm) \n",
    "print(counter_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to Dmatrix format\n",
    "D_train = xgb.DMatrix(X_train_sm, label=y_train_sm)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameters for XGBoost Model\n",
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 2} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds]) ### explain\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds))) ## explain.. adjusting averag='macro' changes this.. look into more\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds))) ### explain\n",
    "print(\"F1-Score: Non-anxiety = {}\".format(f1_score(y_test, best_preds, pos_label=0)))\n",
    "print(\"F1-Score: Anxiety = {}\".format(f1_score(y_test, best_preds, pos_label=1)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit: Autism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit_autism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample the minority class using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42) \n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target after oversampling the minority class using the SMOTE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_resampled = Counter(y_train_sm) \n",
    "print(counter_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the re-sampled features to Dmatrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train_sm, label=y_train_sm)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Parameters for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {   \n",
    "    'num_class': 3} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:12:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.0\n",
      "Recall = 0.0\n",
      "F1-Score: Non-autism = 0.9408625504188644\n",
      "F1-Score: Autism = 0.0\n",
      "Accuracy = 0.8883290367940004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds]) ### explain\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds))) ## explain.. adjusting averag='macro' changes this.. look into more\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds))) ### explain\n",
    "print(\"F1-Score: Non-autism = {}\".format(f1_score(y_test, best_preds, pos_label=0)))\n",
    "print(\"F1-Score: Autism = {}\".format(f1_score(y_test, best_preds, pos_label=1)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit: BPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit_BPD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85338,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 60825, 1: 7445})\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample the minority class using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42) \n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target after oversampling the minority class using the SMOTE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 60825, 0: 60825})\n"
     ]
    }
   ],
   "source": [
    "counter_resampled = Counter(y_train_sm) \n",
    "print(counter_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the re-sampled features to Dmatrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train_sm, label=y_train_sm)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Parameters for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'binary:logistic',  \n",
    "    'num_class': 2} \n",
    "\n",
    "steps = 10  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[08:17:02] /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:57: Check failed: preds.Size() == info.labels_.Size() (243300 vs. 121650) :  labels are not correctly providedpreds.size=243300, label.size=121650, Loss: binary:logistic\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000001b376ed160 dmlc::LogMessageFatal::~LogMessageFatal() + 112\n  [bt] (1) 2   libxgboost.dylib                    0x0000001b377f0f8f xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*) + 447\n  [bt] (2) 3   libxgboost.dylib                    0x0000001b37798b88 xgboost::LearnerImpl::UpdateOneIter(int, std::__1::shared_ptr<xgboost::DMatrix>) + 664\n  [bt] (3) 4   libxgboost.dylib                    0x0000001b376e28cc XGBoosterUpdateOneIter + 156\n  [bt] (4) 5   libffi.6.dylib                      0x000000010afc1884 ffi_call_unix64 + 76\n  [bt] (5) 6   ???                                 0x00007ffee67d3940 0x0 + 140732765387072\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-5288ac557fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    233\u001b[0m                           \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                           \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                           early_stopping_rounds=early_stopping_rounds)\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1280\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[1;32m   1281\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1283\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \"\"\"\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [08:17:02] /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:57: Check failed: preds.Size() == info.labels_.Size() (243300 vs. 121650) :  labels are not correctly providedpreds.size=243300, label.size=121650, Loss: binary:logistic\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000001b376ed160 dmlc::LogMessageFatal::~LogMessageFatal() + 112\n  [bt] (1) 2   libxgboost.dylib                    0x0000001b377f0f8f xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*) + 447\n  [bt] (2) 3   libxgboost.dylib                    0x0000001b37798b88 xgboost::LearnerImpl::UpdateOneIter(int, std::__1::shared_ptr<xgboost::DMatrix>) + 664\n  [bt] (3) 4   libxgboost.dylib                    0x0000001b376e28cc XGBoosterUpdateOneIter + 156\n  [bt] (4) 5   libffi.6.dylib                      0x000000010afc1884 ffi_call_unix64 + 76\n  [bt] (5) 6   ???                                 0x00007ffee67d3940 0x0 + 140732765387072\n\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.7504012841091493\n",
      "Recall = 0.4931434599156118\n",
      "F1-Score: Non-BPD = 0.9589597986707106\n",
      "F1-Score: BPD = 0.5951623169955442\n",
      "Accuracy = 0.9254745722990392\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds]) ### explain\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds))) ## explain.. adjusting averag='macro' changes this.. look into more\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds))) ### explain\n",
    "print(\"F1-Score: Non-BPD = {}\".format(f1_score(y_test, best_preds, pos_label=0)))\n",
    "print(\"F1-Score: BPD = {}\".format(f1_score(y_test, best_preds, pos_label=1)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit: Bipolar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit_bipolar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample the minority class using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42) \n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target after oversampling the minority class using the SMOTE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_resampled = Counter(y_train_sm) \n",
    "print(counter_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the re-sampled features to Dmatrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train_sm, label=y_train_sm)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Parameters for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds]) ### explain\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds))) ## explain.. adjusting averag='macro' changes this.. look into more\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds))) ### explain\n",
    "print(\"F1-Score: Non-bipolar = {}\".format(f1_score(y_test, best_preds, pos_label=0)))\n",
    "print(\"F1-Score: Bipolar = {}\".format(f1_score(y_test, best_preds, pos_label=1)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit: Bulimia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit_bulimia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample the minority class using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42) \n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target after oversampling the minority class using the SMOTE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_resampled = Counter(y_train_sm) \n",
    "print(counter_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the re-sampled features to Dmatrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train_sm, label=y_train_sm)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Parameters for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds]) ### explain\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds))) ## explain.. adjusting averag='macro' changes this.. look into more\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds))) ### explain\n",
    "print(\"F1-Score: Non-bulimia = {}\".format(f1_score(y_test, best_preds, pos_label=0)))\n",
    "print(\"F1-Score: Bulimia = {}\".format(f1_score(y_test, best_preds, pos_label=1)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit: Depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit_depression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample the minority class using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42) \n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target after oversampling the minority class using the SMOTE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_resampled = Counter(y_train_sm) \n",
    "print(counter_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the re-sampled features to Dmatrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train_sm, label=y_train_sm)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Parameters for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds]) ### explain\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds))) ## explain.. adjusting averag='macro' changes this.. look into more\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds))) ### explain\n",
    "print(\"F1-Score: Non-depression = {}\".format(f1_score(y_test, best_preds, pos_label=0)))\n",
    "print(\"F1-Score: Depression = {}\".format(f1_score(y_test, best_preds, pos_label=1)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit: Schizophrenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit_schizophrenia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample the minority class using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42) \n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target after oversampling the minority class using the SMOTE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_resampled = Counter(y_train_sm) \n",
    "print(counter_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the re-sampled features to Dmatrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train_sm, label=y_train_sm)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Parameters for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds]) ### explain\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds))) ## explain.. adjusting averag='macro' changes this.. look into more\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds))) ### explain\n",
    "print(\"F1-Score: Non-schizophrenia = {}\".format(f1_score(y_test, best_preds, pos_label=0)))\n",
    "print(\"F1-Score: Schizophrenia = {}\".format(f1_score(y_test, best_preds, pos_label=1)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try XGBoost with all classes in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
