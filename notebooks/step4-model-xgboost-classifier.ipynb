{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of notebook, from Nature paper will classify each subreddit... etc.\n",
    "\n",
    "> Modeling steps in this notebook\n",
    "- TF-IDF Transform \n",
    "- Split data to training (80%) and testing (20%)\n",
    "- To handle imbalanced class of our target variable we use SMOTE algorithm on the training data\n",
    "- Use XGBoost Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Train, test, split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# For handling imbalanced classes\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# For classification\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/posts-preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>timeframe</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>datetime</th>\n",
       "      <th>words</th>\n",
       "      <th>word_stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>853</td>\n",
       "      <td>sub17967</td>\n",
       "      <td>bulimia</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>['how can i stop hating myself  i have been on...</td>\n",
       "      <td>12749</td>\n",
       "      <td>2444</td>\n",
       "      <td>2017-12-02 16:36:16</td>\n",
       "      <td>['[', \"'how\", 'stop', 'hating', 'eating', 'dis...</td>\n",
       "      <td>['[', \"'how\", 'stop', 'hating', 'eating', 'dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>852</td>\n",
       "      <td>sub10311</td>\n",
       "      <td>bulimia</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>['new guy here 1 month on it 16m  hi guys just...</td>\n",
       "      <td>11906</td>\n",
       "      <td>2334</td>\n",
       "      <td>2017-12-05 19:45:25</td>\n",
       "      <td>['[', \"'new\", 'guy', '1', 'month', '16m', 'hi'...</td>\n",
       "      <td>['[', \"'new\", 'guy', '1', 'month', '16m', 'hi'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>851</td>\n",
       "      <td>sub5587</td>\n",
       "      <td>bulimia</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>['so i just vomited blood what can i eat while...</td>\n",
       "      <td>10688</td>\n",
       "      <td>2051</td>\n",
       "      <td>2017-12-06 16:58:16</td>\n",
       "      <td>['[', \"'so\", 'vomited', 'blood', 'eat', 'throa...</td>\n",
       "      <td>['[', \"'so\", 'vomited', 'blood', 'eat', 'throa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>850</td>\n",
       "      <td>sub32498</td>\n",
       "      <td>bulimia</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>['recovery is expensive  during recovery  hi i...</td>\n",
       "      <td>6027</td>\n",
       "      <td>1125</td>\n",
       "      <td>2017-12-07 14:07:27</td>\n",
       "      <td>['[', \"'recovery\", 'expensive', 'recovery', 'h...</td>\n",
       "      <td>['[', \"'recovery\", 'expensive', 'recovery', 'h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>849</td>\n",
       "      <td>sub35262</td>\n",
       "      <td>bulimia</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>['anyone relate wanting validation for small s...</td>\n",
       "      <td>16805</td>\n",
       "      <td>3164</td>\n",
       "      <td>2017-12-08 00:49:23</td>\n",
       "      <td>['[', \"'anyone\", 'relate', 'wanting', 'validat...</td>\n",
       "      <td>['[', \"'anyone\", 'relate', 'wanting', 'validat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    author subreddit  timeframe  \\\n",
       "0         853  sub17967   bulimia  pre-covid   \n",
       "1         852  sub10311   bulimia  pre-covid   \n",
       "2         851   sub5587   bulimia  pre-covid   \n",
       "3         850  sub32498   bulimia  pre-covid   \n",
       "4         849  sub35262   bulimia  pre-covid   \n",
       "\n",
       "                                                text  text_length  \\\n",
       "0  ['how can i stop hating myself  i have been on...        12749   \n",
       "1  ['new guy here 1 month on it 16m  hi guys just...        11906   \n",
       "2  ['so i just vomited blood what can i eat while...        10688   \n",
       "3  ['recovery is expensive  during recovery  hi i...         6027   \n",
       "4  ['anyone relate wanting validation for small s...        16805   \n",
       "\n",
       "   text_word_count             datetime  \\\n",
       "0             2444  2017-12-02 16:36:16   \n",
       "1             2334  2017-12-05 19:45:25   \n",
       "2             2051  2017-12-06 16:58:16   \n",
       "3             1125  2017-12-07 14:07:27   \n",
       "4             3164  2017-12-08 00:49:23   \n",
       "\n",
       "                                               words  \\\n",
       "0  ['[', \"'how\", 'stop', 'hating', 'eating', 'dis...   \n",
       "1  ['[', \"'new\", 'guy', '1', 'month', '16m', 'hi'...   \n",
       "2  ['[', \"'so\", 'vomited', 'blood', 'eat', 'throa...   \n",
       "3  ['[', \"'recovery\", 'expensive', 'recovery', 'h...   \n",
       "4  ['[', \"'anyone\", 'relate', 'wanting', 'validat...   \n",
       "\n",
       "                                          word_stems  \n",
       "0  ['[', \"'how\", 'stop', 'hating', 'eating', 'dis...  \n",
       "1  ['[', \"'new\", 'guy', '1', 'month', '16m', 'hi'...  \n",
       "2  ['[', \"'so\", 'vomited', 'blood', 'eat', 'throa...  \n",
       "3  ['[', \"'recovery\", 'expensive', 'recovery', 'h...  \n",
       "4  ['[', \"'anyone\", 'relate', 'wanting', 'validat...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binarize targets using get_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use each subreddit as target (except mental health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>timeframe</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>datetime</th>\n",
       "      <th>words</th>\n",
       "      <th>word_stems</th>\n",
       "      <th>subreddit_AnorexiaNervosa</th>\n",
       "      <th>subreddit_BPD</th>\n",
       "      <th>subreddit_autism</th>\n",
       "      <th>subreddit_bulimia</th>\n",
       "      <th>subreddit_schizophrenia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>853</td>\n",
       "      <td>sub17967</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>['how can i stop hating myself  i have been on...</td>\n",
       "      <td>12749</td>\n",
       "      <td>2444</td>\n",
       "      <td>2017-12-02 16:36:16</td>\n",
       "      <td>['[', \"'how\", 'stop', 'hating', 'eating', 'dis...</td>\n",
       "      <td>['[', \"'how\", 'stop', 'hating', 'eating', 'dis...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>852</td>\n",
       "      <td>sub10311</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>['new guy here 1 month on it 16m  hi guys just...</td>\n",
       "      <td>11906</td>\n",
       "      <td>2334</td>\n",
       "      <td>2017-12-05 19:45:25</td>\n",
       "      <td>['[', \"'new\", 'guy', '1', 'month', '16m', 'hi'...</td>\n",
       "      <td>['[', \"'new\", 'guy', '1', 'month', '16m', 'hi'...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>851</td>\n",
       "      <td>sub5587</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>['so i just vomited blood what can i eat while...</td>\n",
       "      <td>10688</td>\n",
       "      <td>2051</td>\n",
       "      <td>2017-12-06 16:58:16</td>\n",
       "      <td>['[', \"'so\", 'vomited', 'blood', 'eat', 'throa...</td>\n",
       "      <td>['[', \"'so\", 'vomited', 'blood', 'eat', 'throa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>850</td>\n",
       "      <td>sub32498</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>['recovery is expensive  during recovery  hi i...</td>\n",
       "      <td>6027</td>\n",
       "      <td>1125</td>\n",
       "      <td>2017-12-07 14:07:27</td>\n",
       "      <td>['[', \"'recovery\", 'expensive', 'recovery', 'h...</td>\n",
       "      <td>['[', \"'recovery\", 'expensive', 'recovery', 'h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>849</td>\n",
       "      <td>sub35262</td>\n",
       "      <td>pre-covid</td>\n",
       "      <td>['anyone relate wanting validation for small s...</td>\n",
       "      <td>16805</td>\n",
       "      <td>3164</td>\n",
       "      <td>2017-12-08 00:49:23</td>\n",
       "      <td>['[', \"'anyone\", 'relate', 'wanting', 'validat...</td>\n",
       "      <td>['[', \"'anyone\", 'relate', 'wanting', 'validat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3871</td>\n",
       "      <td>27445</td>\n",
       "      <td>sub15361</td>\n",
       "      <td>post-covid</td>\n",
       "      <td>['idfk  relapsed into my bulimia purging every...</td>\n",
       "      <td>3466</td>\n",
       "      <td>646</td>\n",
       "      <td>2021-02-18 18:12:13</td>\n",
       "      <td>['[', \"'idfk\", 'relapsed', 'purging', 'every',...</td>\n",
       "      <td>['[', \"'idfk\", 'relapsed', 'purging', 'every',...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3872</td>\n",
       "      <td>27441</td>\n",
       "      <td>sub13955</td>\n",
       "      <td>post-covid</td>\n",
       "      <td>['well i managed a whole two weeks  was on vac...</td>\n",
       "      <td>9904</td>\n",
       "      <td>1899</td>\n",
       "      <td>2021-02-18 22:54:47</td>\n",
       "      <td>['[', \"'well\", 'managed', 'whole', 'two', 'wee...</td>\n",
       "      <td>['[', \"'well\", 'managed', 'whole', 'two', 'wee...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3873</td>\n",
       "      <td>27439</td>\n",
       "      <td>sub22575</td>\n",
       "      <td>post-covid</td>\n",
       "      <td>['first day in a long time with no purging  ju...</td>\n",
       "      <td>6651</td>\n",
       "      <td>1233</td>\n",
       "      <td>2021-02-18 23:45:43</td>\n",
       "      <td>['[', \"'first\", 'day', 'long', 'time', 'purgin...</td>\n",
       "      <td>['[', \"'first\", 'day', 'long', 'time', 'purgin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3874</td>\n",
       "      <td>25306</td>\n",
       "      <td>sub2340</td>\n",
       "      <td>post-covid</td>\n",
       "      <td>['bingingpurging recovery  stomach issues  so ...</td>\n",
       "      <td>7247</td>\n",
       "      <td>1416</td>\n",
       "      <td>2021-02-23 15:49:58</td>\n",
       "      <td>['[', \"'bingingpurging\", 'recovery', 'stomach'...</td>\n",
       "      <td>['[', \"'bingingpurging\", 'recovery', 'stomach'...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3875</td>\n",
       "      <td>31813</td>\n",
       "      <td>sub10098</td>\n",
       "      <td>post-covid</td>\n",
       "      <td>['do you feel you cant understand anything  do...</td>\n",
       "      <td>8577</td>\n",
       "      <td>1607</td>\n",
       "      <td>2021-02-26 10:45:04</td>\n",
       "      <td>['[', \"'do\", 'feel', 'cant', 'understand', 'an...</td>\n",
       "      <td>['[', \"'do\", 'feel', 'cant', 'understand', 'an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3876 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0    author   timeframe  \\\n",
       "0            853  sub17967   pre-covid   \n",
       "1            852  sub10311   pre-covid   \n",
       "2            851   sub5587   pre-covid   \n",
       "3            850  sub32498   pre-covid   \n",
       "4            849  sub35262   pre-covid   \n",
       "...          ...       ...         ...   \n",
       "3871       27445  sub15361  post-covid   \n",
       "3872       27441  sub13955  post-covid   \n",
       "3873       27439  sub22575  post-covid   \n",
       "3874       25306   sub2340  post-covid   \n",
       "3875       31813  sub10098  post-covid   \n",
       "\n",
       "                                                   text  text_length  \\\n",
       "0     ['how can i stop hating myself  i have been on...        12749   \n",
       "1     ['new guy here 1 month on it 16m  hi guys just...        11906   \n",
       "2     ['so i just vomited blood what can i eat while...        10688   \n",
       "3     ['recovery is expensive  during recovery  hi i...         6027   \n",
       "4     ['anyone relate wanting validation for small s...        16805   \n",
       "...                                                 ...          ...   \n",
       "3871  ['idfk  relapsed into my bulimia purging every...         3466   \n",
       "3872  ['well i managed a whole two weeks  was on vac...         9904   \n",
       "3873  ['first day in a long time with no purging  ju...         6651   \n",
       "3874  ['bingingpurging recovery  stomach issues  so ...         7247   \n",
       "3875  ['do you feel you cant understand anything  do...         8577   \n",
       "\n",
       "      text_word_count             datetime  \\\n",
       "0                2444  2017-12-02 16:36:16   \n",
       "1                2334  2017-12-05 19:45:25   \n",
       "2                2051  2017-12-06 16:58:16   \n",
       "3                1125  2017-12-07 14:07:27   \n",
       "4                3164  2017-12-08 00:49:23   \n",
       "...               ...                  ...   \n",
       "3871              646  2021-02-18 18:12:13   \n",
       "3872             1899  2021-02-18 22:54:47   \n",
       "3873             1233  2021-02-18 23:45:43   \n",
       "3874             1416  2021-02-23 15:49:58   \n",
       "3875             1607  2021-02-26 10:45:04   \n",
       "\n",
       "                                                  words  \\\n",
       "0     ['[', \"'how\", 'stop', 'hating', 'eating', 'dis...   \n",
       "1     ['[', \"'new\", 'guy', '1', 'month', '16m', 'hi'...   \n",
       "2     ['[', \"'so\", 'vomited', 'blood', 'eat', 'throa...   \n",
       "3     ['[', \"'recovery\", 'expensive', 'recovery', 'h...   \n",
       "4     ['[', \"'anyone\", 'relate', 'wanting', 'validat...   \n",
       "...                                                 ...   \n",
       "3871  ['[', \"'idfk\", 'relapsed', 'purging', 'every',...   \n",
       "3872  ['[', \"'well\", 'managed', 'whole', 'two', 'wee...   \n",
       "3873  ['[', \"'first\", 'day', 'long', 'time', 'purgin...   \n",
       "3874  ['[', \"'bingingpurging\", 'recovery', 'stomach'...   \n",
       "3875  ['[', \"'do\", 'feel', 'cant', 'understand', 'an...   \n",
       "\n",
       "                                             word_stems  \\\n",
       "0     ['[', \"'how\", 'stop', 'hating', 'eating', 'dis...   \n",
       "1     ['[', \"'new\", 'guy', '1', 'month', '16m', 'hi'...   \n",
       "2     ['[', \"'so\", 'vomited', 'blood', 'eat', 'throa...   \n",
       "3     ['[', \"'recovery\", 'expensive', 'recovery', 'h...   \n",
       "4     ['[', \"'anyone\", 'relate', 'wanting', 'validat...   \n",
       "...                                                 ...   \n",
       "3871  ['[', \"'idfk\", 'relapsed', 'purging', 'every',...   \n",
       "3872  ['[', \"'well\", 'managed', 'whole', 'two', 'wee...   \n",
       "3873  ['[', \"'first\", 'day', 'long', 'time', 'purgin...   \n",
       "3874  ['[', \"'bingingpurging\", 'recovery', 'stomach'...   \n",
       "3875  ['[', \"'do\", 'feel', 'cant', 'understand', 'an...   \n",
       "\n",
       "      subreddit_AnorexiaNervosa  subreddit_BPD  subreddit_autism  \\\n",
       "0                             0              0                 0   \n",
       "1                             0              0                 0   \n",
       "2                             0              0                 0   \n",
       "3                             0              0                 0   \n",
       "4                             0              0                 0   \n",
       "...                         ...            ...               ...   \n",
       "3871                          0              0                 0   \n",
       "3872                          0              0                 0   \n",
       "3873                          0              0                 0   \n",
       "3874                          0              0                 0   \n",
       "3875                          0              0                 1   \n",
       "\n",
       "      subreddit_bulimia  subreddit_schizophrenia  \n",
       "0                     1                        0  \n",
       "1                     1                        0  \n",
       "2                     1                        0  \n",
       "3                     1                        0  \n",
       "4                     1                        0  \n",
       "...                 ...                      ...  \n",
       "3871                  1                        0  \n",
       "3872                  1                        0  \n",
       "3873                  1                        0  \n",
       "3874                  1                        0  \n",
       "3875                  0                        0  \n",
       "\n",
       "[3876 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['subreddit_mentalhealth'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c3d91bf1868d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'subreddit_mentalhealth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m         )\n\u001b[1;32m   4104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3912\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3913\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3914\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3944\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3945\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3946\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3947\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5340\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not found in axis\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5341\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['subreddit_mentalhealth'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df.drop(columns='subreddit_mentalhealth', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize using TFIDF\n",
    "Implement Term Frequency - Inverse Document Frequency (TF-IDF) to vectorize the pre-processed text from the subreddit posts into numerical representations in a weight matrix that will be the basis for our set of feature for the predictive models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit: Anorexia Nervosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit_AnorexiaNervosa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample the minority class using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42) \n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target after oversampling the minority class using the SMOTE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_resampled = Counter(y_train_sm) \n",
    "print(counter_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the re-sampled features to Dmatrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train_sm, label=y_train_sm)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Parameters for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds]) ### explain\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds))) ## explain.. adjusting averag='macro' changes this.. look into more\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds))) ### explain\n",
    "print(\"F1-Score: Non-anorexia = {}\".format(f1_score(y_test, best_preds, pos_label=0)))\n",
    "print(\"F1-Score: Anorexia = {}\".format(f1_score(y_test, best_preds, pos_label=1)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Grid search... may be a waste of time... run over night and see what params we get\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "parameters = {\n",
    "     \"eta\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "     \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    "     \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "     \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "     \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n",
    "     }\n",
    "\n",
    "grid = GridSearchCV(clf,\n",
    "                    parameters, n_jobs=4,\n",
    "                    scoring=\"neg_log_loss\",\n",
    "                    cv=3)\n",
    "\n",
    "grid.fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit: Anxiety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit_Anxiety']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize class distribution\n",
    "counter = Counter(y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample the minority class using SMOTE\n",
    "smote = SMOTE(random_state=42) \n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize class distribution\n",
    "counter_resampled = Counter(y_train_sm) \n",
    "print(counter_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to Dmatrix format\n",
    "D_train = xgb.DMatrix(X_train_sm, label=y_train_sm)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameters for XGBoost Model\n",
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds]) ### explain\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds))) ## explain.. adjusting averag='macro' changes this.. look into more\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds))) ### explain\n",
    "print(\"F1-Score: Non-anxiety = {}\".format(f1_score(y_test, best_preds, pos_label=0)))\n",
    "print(\"F1-Score: Anxiety = {}\".format(f1_score(y_test, best_preds, pos_label=1)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit: Autism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit_autism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample the minority class using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42) \n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target after oversampling the minority class using the SMOTE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_resampled = Counter(y_train_sm) \n",
    "print(counter_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the re-sampled features to Dmatrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train_sm, label=y_train_sm)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Parameters for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds]) ### explain\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds))) ## explain.. adjusting averag='macro' changes this.. look into more\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds))) ### explain\n",
    "print(\"F1-Score: Non-autism = {}\".format(f1_score(y_test, best_preds, pos_label=0)))\n",
    "print(\"F1-Score: Autism = {}\".format(f1_score(y_test, best_preds, pos_label=1)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit: BPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit_BPD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample the minority class using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42) \n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target after oversampling the minority class using the SMOTE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_resampled = Counter(y_train_sm) \n",
    "print(counter_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the re-sampled features to Dmatrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train_sm, label=y_train_sm)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Parameters for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds]) ### explain\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds))) ## explain.. adjusting averag='macro' changes this.. look into more\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds))) ### explain\n",
    "print(\"F1-Score: Non-BPD = {}\".format(f1_score(y_test, best_preds, pos_label=0)))\n",
    "print(\"F1-Score: BPD = {}\".format(f1_score(y_test, best_preds, pos_label=1)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit: Bipolar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit_bipolar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample the minority class using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42) \n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target after oversampling the minority class using the SMOTE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_resampled = Counter(y_train_sm) \n",
    "print(counter_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the re-sampled features to Dmatrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train_sm, label=y_train_sm)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Parameters for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds]) ### explain\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds))) ## explain.. adjusting averag='macro' changes this.. look into more\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds))) ### explain\n",
    "print(\"F1-Score: Non-bipolar = {}\".format(f1_score(y_test, best_preds, pos_label=0)))\n",
    "print(\"F1-Score: Bipolar = {}\".format(f1_score(y_test, best_preds, pos_label=1)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit: Bulimia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit_bulimia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample the minority class using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42) \n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target after oversampling the minority class using the SMOTE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_resampled = Counter(y_train_sm) \n",
    "print(counter_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the re-sampled features to Dmatrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train_sm, label=y_train_sm)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Parameters for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds]) ### explain\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds))) ## explain.. adjusting averag='macro' changes this.. look into more\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds))) ### explain\n",
    "print(\"F1-Score: Non-bulimia = {}\".format(f1_score(y_test, best_preds, pos_label=0)))\n",
    "print(\"F1-Score: Bulimia = {}\".format(f1_score(y_test, best_preds, pos_label=1)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit: Depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit_depression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample the minority class using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42) \n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target after oversampling the minority class using the SMOTE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_resampled = Counter(y_train_sm) \n",
    "print(counter_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the re-sampled features to Dmatrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train_sm, label=y_train_sm)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Parameters for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds]) ### explain\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds))) ## explain.. adjusting averag='macro' changes this.. look into more\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds))) ### explain\n",
    "print(\"F1-Score: Non-depression = {}\".format(f1_score(y_test, best_preds, pos_label=0)))\n",
    "print(\"F1-Score: Depression = {}\".format(f1_score(y_test, best_preds, pos_label=1)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit: Schizophrenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit_schizophrenia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = .2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_train)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample the minority class using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42) \n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the class distribution of the target after oversampling the minority class using the SMOTE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_resampled = Counter(y_train_sm) \n",
    "print(counter_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the re-sampled features to Dmatrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train_sm, label=y_train_sm)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Parameters for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds]) ### explain\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds))) ## explain.. adjusting averag='macro' changes this.. look into more\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds))) ### explain\n",
    "print(\"F1-Score: Non-schizophrenia = {}\".format(f1_score(y_test, best_preds, pos_label=0)))\n",
    "print(\"F1-Score: Schizophrenia = {}\".format(f1_score(y_test, best_preds, pos_label=1)))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
